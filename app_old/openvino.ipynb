{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub --quiet\n",
    "%pip install transformers --quiet\n",
    "%pip install torch torchvision torchaudio --quiet\n",
    "%pip install --upgrade pip --quiet\n",
    "%pip install tensorflow --quiet\n",
    "%pip install python-dotenv --quiet\n",
    "%pip install --upgrade --upgrade-strategy eager \"optimum[openvino]\" --quiet\n",
    "%pip install tf-keras --quiet\n",
    "%pip install sentence-transformers --quiet\n",
    "%pip install langchain_community --quiet\n",
    "%pip install langchain_openai --quiet\n",
    "%pip install pypdf --quiet\n",
    "%pip install chromadb --quiet\n",
    "%pip install langchain-chroma --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import logging\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from huggingface_hub import login, whoami\n",
    "from optimum.intel import OVQuantizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "import openvino as ov\n",
    "import nncf\n",
    "\n",
    "nncf.set_log_level(logging.ERROR)\n",
    "\n",
    "load_dotenv(verbose=True)\n",
    "cache_dir = os.environ['CACHE_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_vendor, model_id, group_size:int, ratio:float, int4_mode:str='SYM', generate_fp16:bool=True, generate_int8:bool=True, generate_int4:bool=True, cache_dir='./cache'):\n",
    "    pt_model_id = f'{model_vendor}/{model_id}'\n",
    "    fp16_model_dir = Path(model_id) / \"FP16\"\n",
    "    int8_model_dir = Path(model_id) / \"INT8\"\n",
    "    int4_model_dir = Path(model_id) / \"INT4\"\n",
    "\n",
    "    ov_model_file_name = 'openvino_model.xml'\n",
    "\n",
    "    print(f'** Prepaing model : {model_vendor}/{model_id}')\n",
    "\n",
    "    # FP16\n",
    "    if generate_fp16 and not os.path.exists(fp16_model_dir / ov_model_file_name):\n",
    "        print('\\n** Generating an FP16 IR model')\n",
    "        ov_model = OVModelForCausalLM.from_pretrained(pt_model_id, export=True, compile=False, cache_dir=cache_dir, ov_config={'CACHE_DIR':cache_dir})\n",
    "        ov_model.half()\n",
    "        ov_model.save_pretrained(fp16_model_dir)\n",
    "        del ov_model\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print('\\n** Skip generation of FP16 IR model (directory already exists)')\n",
    "\n",
    "    # INT8\n",
    "    if generate_int8 and not os.path.exists(int8_model_dir / ov_model_file_name):\n",
    "        print('\\n** Generating an INT8 IR model')\n",
    "        ov_model = OVModelForCausalLM.from_pretrained(fp16_model_dir, compile=False, cache_dir=cache_dir, ov_config={'CACHE_DIR':cache_dir})\n",
    "        quantizer = OVQuantizer.from_pretrained(ov_model, cache_dir=cache_dir)\n",
    "        quantizer.quantize(save_directory=int8_model_dir, weights_only=True)\n",
    "        del quantizer\n",
    "        del ov_model\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print('\\n** Skip generation of INT8 IR model (directory already exists)')\n",
    "\n",
    "    # INT4\n",
    "    if generate_int4 and not os.path.exists(int4_model_dir / ov_model_file_name):\n",
    "        print(f'\\n** Generating an INT4_{int4_mode} IR model')\n",
    "        ov_model = OVModelForCausalLM.from_pretrained(fp16_model_dir, compile=False, cache_dir=cache_dir, ov_config={'CACHE_DIR':cache_dir})\n",
    "        int4_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        ov_model = ov.Core().read_model(fp16_model_dir / ov_model_file_name)\n",
    "        shutil.copy(fp16_model_dir / 'config.json', int4_model_dir / 'config.json')\n",
    "        comp_mode = nncf.CompressWeightsMode.INT4_ASYM if int4_mode=='ASYM' else nncf.CompressWeightsMode.INT4_SYM\n",
    "        compressed_model = nncf.compress_weights(ov_model, mode=comp_mode, ratio=ratio, group_size=group_size)\n",
    "        ov.save_model(compressed_model, int4_model_dir / ov_model_file_name)\n",
    "        del ov_model\n",
    "        del compressed_model\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print('\\n** Skip generation of INT4 IR model (directory already exists)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_token = hf_xxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*** LLM model downloader')\n",
    "try:\n",
    "    whoami()\n",
    "    print('Authorization token already provided')\n",
    "except OSError:\n",
    "    print('The llama2 model is a controlled model.')\n",
    "    print('You need to login to HuggingFace hub to download the model.')\n",
    "    login()\n",
    "finally:\n",
    "    prepare_model('meta-llama', 'Llama-3.2-1B', group_size=128, ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import  DirectoryLoader, PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] ='http://10.35.151.101:8001/v1'\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-1234\"\n",
    "CHROMA_PATH = \"docs_embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "  document_loader = PyPDFDirectoryLoader('docs') \n",
    "  return document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, # Size of each chunk in characters\n",
    "    chunk_overlap=100, # Overlap between consecutive chunks\n",
    "    length_function=len, # Function to compute the length of the text\n",
    "    add_start_index=True, # Flag to add start index to each chunk\n",
    "  )\n",
    "\n",
    "  chunks = text_splitter.split_documents(documents)\n",
    "  print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "  document = chunks[0]\n",
    "  print(document.page_content)\n",
    "  print(document.metadata)\n",
    "\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_chroma(chunks: list[Document]):\n",
    "  if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "    db = Chroma.from_documents(\n",
    "      chunks,\n",
    "      OpenAIEmbeddings(),\n",
    "      persist_directory=CHROMA_PATH\n",
    "    )\n",
    "\n",
    "    db.persist()\n",
    "  print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_store():\n",
    "  documents = load_documents()\n",
    "  chunks = split_text(documents)\n",
    "  save_to_chroma(chunks)\n",
    "\n",
    "print('*** Converting documents into embeddings and creating a vector store(s)')\n",
    "generate_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\", api_url=\"https://api.hub.langchain.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "vectorstore_dir   = os.environ['VECTOR_DB_DIR']\n",
    "embeddings_model  = os.environ['MODEL_EMBEDDINGS']\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    " - -\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "vectorstore_dir = f'{vectorstore_dir}'\n",
    "vectorstore = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "#    search_type='similarity_score_threshold', \n",
    "#    search_kwargs={\n",
    "#        'score_threshold' : 0.8, \n",
    "#        'k' : 4\n",
    "#    }\n",
    "#)\n",
    "\n",
    "# results = vectorstore.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "\n",
    "# if len(results) == 0 or results[0][1] < 0.7:\n",
    "#    print(f\"Unable to find matching results.\")\n",
    "\n",
    "# context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "# prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "print(f'** Vector store : {vectorstore_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "model_id = 'Llama-3.2-1B'\n",
    "model_name        = os.environ['MODEL_NAME']\n",
    "model_precision   = os.environ['MODEL_PRECISION']\n",
    "inference_device  = os.environ['INFERENCE_DEVICE']\n",
    "ov_config         = {\"PERFORMANCE_HINT\":\"LATENCY\", \"NUM_STREAMS\":\"1\", \"CACHE_DIR\":cache_dir}\n",
    "num_max_tokens    = int(os.environ['NUM_MAX_TOKENS'])\n",
    "rag_chain_type    = os.environ['RAG_CHAIN_TYPE']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "ov_model_path = f'./{model_name}/{model_precision}'\n",
    "model = OVModelForCausalLM.from_pretrained(model_id=ov_model_path, device=inference_device, ov_config=ov_config, cache_dir=cache_dir)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=num_max_tokens)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=rag_chain_type, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_user_en = \"Who administrates the Promotions?\"\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "model_id = 'Llama-3.2-1B'\n",
    "model_name        = os.environ['MODEL_NAME']\n",
    "model_precision   = \"INT4\"\n",
    "inference_device  = os.environ['INFERENCE_DEVICE']\n",
    "ov_config         = {\"PERFORMANCE_HINT\":\"LATENCY\", \"NUM_STREAMS\":\"1\", \"CACHE_DIR\":cache_dir}\n",
    "num_max_tokens    = 200\n",
    "rag_chain_type    = os.environ['RAG_CHAIN_TYPE']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "ov_model_path = f'./{model_name}/{model_precision}'\n",
    "model = OVModelForCausalLM.from_pretrained(model_id=ov_model_path, device=inference_device, ov_config=ov_config, cache_dir=cache_dir)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=num_max_tokens)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=rag_chain_type, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_user_en = \"What is the promotion policy for non teaching staff?\"\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Int 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "model_id = 'Llama-3.2-1B'\n",
    "model_name        = os.environ['MODEL_NAME']\n",
    "model_precision   = \"INT8\"\n",
    "inference_device  = os.environ['INFERENCE_DEVICE']\n",
    "ov_config         = {\"PERFORMANCE_HINT\":\"LATENCY\", \"NUM_STREAMS\":\"1\", \"CACHE_DIR\":cache_dir}\n",
    "num_max_tokens    = 100\n",
    "rag_chain_type    = os.environ['RAG_CHAIN_TYPE']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "ov_model_path = f'./{model_name}/{model_precision}'\n",
    "model = OVModelForCausalLM.from_pretrained(model_id=ov_model_path, device=inference_device, ov_config=ov_config, cache_dir=cache_dir)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=num_max_tokens)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=rag_chain_type, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_user_en = \"Can you tell me about the staff referal incentive policy\"\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What is the promotion policy for non teaching staff?\"\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "\n",
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "model_id = 'Llama-3.2-1B'\n",
    "model_name        = os.environ['MODEL_NAME']\n",
    "model_precision   = \"INT8\"\n",
    "inference_device  = os.environ['INFERENCE_DEVICE']\n",
    "ov_config         = {\"PERFORMANCE_HINT\":\"LATENCY\", \"NUM_STREAMS\":\"1\", \"CACHE_DIR\":cache_dir}\n",
    "num_max_tokens    = 200\n",
    "rag_chain_type    = os.environ['RAG_CHAIN_TYPE']\n",
    "PROMPT_TEMPLATE   = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    " - -\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "# prompt = \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "ov_model_path = f'./{model_name}/{model_precision}'\n",
    "model = OVModelForCausalLM.from_pretrained(model_id=ov_model_path, device=inference_device, ov_config=ov_config, cache_dir=cache_dir)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=num_max_tokens)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "chain = prompt_template | llm\n",
    "# print(chain.invoke({\"context\": context_text,\"question\": query_text}))\n",
    "response_text = llm.predict(prompt)\n",
    "# qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=rag_chain_type, retriever=retriever, chain_type_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "model_id = 'Llama-3.2-1B'\n",
    "model_name        = os.environ['MODEL_NAME']\n",
    "model_precision   = \"FP16\"\n",
    "inference_device  = os.environ['INFERENCE_DEVICE']\n",
    "ov_config         = {\"PERFORMANCE_HINT\":\"LATENCY\", \"NUM_STREAMS\":\"1\", \"CACHE_DIR\":cache_dir}\n",
    "num_max_tokens    = 200\n",
    "rag_chain_type    = os.environ['RAG_CHAIN_TYPE']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "ov_model_path = f'./{model_name}/{model_precision}'\n",
    "model = OVModelForCausalLM.from_pretrained(model_id=ov_model_path, device=inference_device, ov_config=ov_config, cache_dir=cache_dir)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=num_max_tokens)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=rag_chain_type, retriever=retriever, chain_type_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_user_en = \"Can you tell me about the Extension of Probation Period\"\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_user_en = \"Can you tell me about the staff referal incentive policy\"\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_user_en = \"Can you tell me about the leave policy?\"\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load and compress model from Hugging Face\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "model = OVModelForCausalLM.from_pretrained(model_id, export=True, load_in_8bit=True)\n",
    "\n",
    "# Inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "phrase = \"The weather is\"\n",
    "results = pipe(phrase)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load and compress model from Hugging Face\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "model = OVModelForCausalLM.from_pretrained(model_id, export=True)\n",
    "\n",
    "# Inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "phrase = \"The weather is\"\n",
    "results = pipe(phrase)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
