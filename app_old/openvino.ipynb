{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub --quiet\n",
    "%pip install transformers --quiet\n",
    "%pip install torch torchvision torchaudio --quiet\n",
    "%pip install --upgrade pip --quiet\n",
    "%pip install tensorflow --quiet\n",
    "%pip install python-dotenv --quiet\n",
    "%pip install --upgrade --upgrade-strategy eager \"optimum[openvino]\" --quiet\n",
    "%pip install tf-keras --quiet\n",
    "%pip install sentence-transformers --quiet\n",
    "%pip install langchain_community --quiet\n",
    "%pip install langchain_openai --quiet\n",
    "%pip install pypdf --quiet\n",
    "%pip install chromadb --quiet\n",
    "%pip install langchain-chroma --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpcuser/joycehcl/openvino/openvino_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-29 07:58:01.898538: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-29 07:58:01.914407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730188681.933902 3577971 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730188681.939446 3577971 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-29 07:58:01.960335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import logging\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from huggingface_hub import login, whoami\n",
    "from optimum.intel import OVQuantizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "import openvino as ov\n",
    "import nncf\n",
    "\n",
    "nncf.set_log_level(logging.ERROR)\n",
    "\n",
    "load_dotenv(verbose=True)\n",
    "cache_dir = os.environ['CACHE_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_vendor, model_id, group_size:int, ratio:float, int4_mode:str='SYM', generate_fp16:bool=True, generate_int8:bool=True, generate_int4:bool=True, cache_dir='./cache'):\n",
    "    pt_model_id = f'{model_vendor}/{model_id}'\n",
    "    fp16_model_dir = Path(model_id) / \"FP16\"\n",
    "    int8_model_dir = Path(model_id) / \"INT8\"\n",
    "    int4_model_dir = Path(model_id) / \"INT4\"\n",
    "\n",
    "    ov_model_file_name = 'openvino_model.xml'\n",
    "\n",
    "    print(f'** Prepaing model : {model_vendor}/{model_id}')\n",
    "\n",
    "    # FP16\n",
    "    if generate_fp16 and not os.path.exists(fp16_model_dir / ov_model_file_name):\n",
    "        print('\\n** Generating an FP16 IR model')\n",
    "        ov_model = OVModelForCausalLM.from_pretrained(pt_model_id, export=True, compile=False, cache_dir=cache_dir, ov_config={'CACHE_DIR':cache_dir})\n",
    "        ov_model.half()\n",
    "        ov_model.save_pretrained(fp16_model_dir)\n",
    "        del ov_model\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print('\\n** Skip generation of FP16 IR model (directory already exists)')\n",
    "\n",
    "    # INT8\n",
    "    if generate_int8 and not os.path.exists(int8_model_dir / ov_model_file_name):\n",
    "        print('\\n** Generating an INT8 IR model')\n",
    "        ov_model = OVModelForCausalLM.from_pretrained(fp16_model_dir, compile=False, cache_dir=cache_dir, ov_config={'CACHE_DIR':cache_dir})\n",
    "        quantizer = OVQuantizer.from_pretrained(ov_model, cache_dir=cache_dir)\n",
    "        quantizer.quantize(save_directory=int8_model_dir, weights_only=True)\n",
    "        del quantizer\n",
    "        del ov_model\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print('\\n** Skip generation of INT8 IR model (directory already exists)')\n",
    "\n",
    "    # INT4\n",
    "    if generate_int4 and not os.path.exists(int4_model_dir / ov_model_file_name):\n",
    "        print(f'\\n** Generating an INT4_{int4_mode} IR model')\n",
    "        ov_model = OVModelForCausalLM.from_pretrained(fp16_model_dir, compile=False, cache_dir=cache_dir, ov_config={'CACHE_DIR':cache_dir})\n",
    "        int4_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        ov_model = ov.Core().read_model(fp16_model_dir / ov_model_file_name)\n",
    "        shutil.copy(fp16_model_dir / 'config.json', int4_model_dir / 'config.json')\n",
    "        comp_mode = nncf.CompressWeightsMode.INT4_ASYM if int4_mode=='ASYM' else nncf.CompressWeightsMode.INT4_SYM\n",
    "        compressed_model = nncf.compress_weights(ov_model, mode=comp_mode, ratio=ratio, group_size=group_size)\n",
    "        ov.save_model(compressed_model, int4_model_dir / ov_model_file_name)\n",
    "        del ov_model\n",
    "        del compressed_model\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print('\\n** Skip generation of INT4 IR model (directory already exists)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_token = hf_xxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** LLM model downloader\n",
      "Authorization token already provided\n",
      "** Prepaing model : meta-llama/Llama-3.2-1B\n",
      "\n",
      "** Generating an FP16 IR model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
      "/home/hpcuser/joycehcl/openvino/openvino_env/lib/python3.10/site-packages/transformers/cache_utils.py:447: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
      "/home/hpcuser/joycehcl/openvino/openvino_env/lib/python3.10/site-packages/optimum/exporters/openvino/model_patcher.py:496: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/home/hpcuser/joycehcl/openvino/openvino_env/lib/python3.10/site-packages/transformers/cache_utils.py:432: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  elif len(self.key_cache[layer_idx]) == 0:  # fills previously skipped layers; checking for tensor causes errors\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/hpcuser/joycehcl/openvino/openvino_env/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/hpcuser/joycehcl/openvino/openvino_env/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Generating an INT8 IR model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`quantization_config` was not provided. In the future, please provide `quantization_config`\n",
      "Calibration dataset was not provided, assuming weight only quantization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Generating an INT4_SYM IR model\n"
     ]
    }
   ],
   "source": [
    "print('*** LLM model downloader')\n",
    "try:\n",
    "    whoami()\n",
    "    print('Authorization token already provided')\n",
    "except OSError:\n",
    "    print('The llama2 model is a controlled model.')\n",
    "    print('You need to login to HuggingFace hub to download the model.')\n",
    "    login()\n",
    "finally:\n",
    "    prepare_model('meta-llama', 'Llama-3.2-1B', group_size=128, ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import  DirectoryLoader, PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] ='http://10.35.151.101:8001/v1'\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-1234\"\n",
    "CHROMA_PATH = \"docs_embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "  document_loader = PyPDFDirectoryLoader('docs') \n",
    "  return document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, # Size of each chunk in characters\n",
    "    chunk_overlap=100, # Overlap between consecutive chunks\n",
    "    length_function=len, # Function to compute the length of the text\n",
    "    add_start_index=True, # Flag to add start index to each chunk\n",
    "  )\n",
    "\n",
    "  chunks = text_splitter.split_documents(documents)\n",
    "  print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "  document = chunks[0]\n",
    "  print(document.page_content)\n",
    "  print(document.metadata)\n",
    "\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_chroma(chunks: list[Document]):\n",
    "  if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "    db = Chroma.from_documents(\n",
    "      chunks,\n",
    "      OpenAIEmbeddings(),\n",
    "      persist_directory=CHROMA_PATH\n",
    "    )\n",
    "\n",
    "    db.persist()\n",
    "  print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Converting documents into embeddings and creating a vector store(s)\n",
      "Split 34 documents into 173 chunks.\n",
      "(Affiliated to C.I.S.C.E, New Delhi)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "HR POLICIES \n",
      "Handbook\n",
      "{'source': 'docs/NHPS-POLICIES-Revised-1.pdf', 'page': 0, 'start_index': 12}\n",
      "Saved 173 chunks to docs_embedding.\n"
     ]
    }
   ],
   "source": [
    "def generate_data_store():\n",
    "  documents = load_documents()\n",
    "  chunks = split_text(documents)\n",
    "  save_to_chroma(chunks)\n",
    "\n",
    "print('*** Converting documents into embeddings and creating a vector store(s)')\n",
    "generate_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpcuser/joycehcl/openvino/openvino_env/lib/python3.10/site-packages/langsmith/client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\", api_url=\"https://api.hub.langchain.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Vector store : docs_embedding\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "vectorstore_dir   = os.environ['VECTOR_DB_DIR']\n",
    "embeddings_model  = os.environ['MODEL_EMBEDDINGS']\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    " - -\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "vectorstore_dir = f'{vectorstore_dir}'\n",
    "vectorstore = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "#    search_type='similarity_score_threshold', \n",
    "#    search_kwargs={\n",
    "#        'score_threshold' : 0.8, \n",
    "#        'k' : 4\n",
    "#    }\n",
    "#)\n",
    "\n",
    "# results = vectorstore.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "\n",
    "# if len(results) == 0 or results[0][1] < 0.7:\n",
    "#    print(f\"Unable to find matching results.\")\n",
    "\n",
    "# context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "# prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "print(f'** Vector store : {vectorstore_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/tmp/ipykernel_3577971/556153035.py:19: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "model_id = 'Llama-3.2-1B'\n",
    "model_name        = os.environ['MODEL_NAME']\n",
    "model_precision   = os.environ['MODEL_PRECISION']\n",
    "inference_device  = os.environ['INFERENCE_DEVICE']\n",
    "ov_config         = {\"PERFORMANCE_HINT\":\"LATENCY\", \"NUM_STREAMS\":\"1\", \"CACHE_DIR\":cache_dir}\n",
    "num_max_tokens    = int(os.environ['NUM_MAX_TOKENS'])\n",
    "rag_chain_type    = os.environ['RAG_CHAIN_TYPE']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "ov_model_path = f'./{model_name}/{model_precision}'\n",
    "model = OVModelForCausalLM.from_pretrained(model_id=ov_model_path, device=inference_device, ov_config=ov_config, cache_dir=cache_dir)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=num_max_tokens)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=rag_chain_type, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3577971/4141815617.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  ans = qa_chain.run(text_user_en)\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "\n",
      "\n",
      "Question: Who administrates the Promotions?\n",
      "Helpful Answer: The Promotions department is responsible for the promotion of the company's products and services. They are responsible for creating and implementing marketing strategies, advertising campaigns, and promotional activities to increase brand awareness and sales. They also work closely with the sales team to ensure that the products are properly marketed and promoted to the target audience. The Promotions department is responsible for managing the company's promotional budget, ensuring that the budget is spent effectively and efficiently, and that the company's promotional activities are aligned with the company's overall marketing strategy. The Promotions department is also responsible for managing the company's social media accounts, creating and managing promotional content, and monitoring the company's social media engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is responsible for managing the company's website, creating and managing promotional content, and monitoring the company's website traffic to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's email marketing campaigns, creating and managing promotional content, and monitoring the company's email marketing engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's print and digital advertising campaigns, creating and managing promotional content, and monitoring the company's print and digital advertising engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's direct mail campaigns, creating and managing promotional content, and monitoring the company's direct mail engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's trade show and event marketing campaigns, creating and managing promotional content, and monitoring the company's trade show and event engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's influencer marketing campaigns, creating and managing promotional content, and monitoring the company's influencer marketing engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's content marketing campaigns, creating and managing promotional content, and monitoring the company's content marketing engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's search engine optimization (SEO) campaigns, creating and managing promotional content, and monitoring the company's SEO engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's social media advertising campaigns, creating and managing promotional content, and monitoring the company's social media advertising engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's email marketing automation campaigns, creating and managing promotional content, and monitoring the company's email marketing automation engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's content marketing automation campaigns, creating and managing promotional content, and monitoring the company's content marketing automation engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's search engine optimization (SEO) automation campaigns, creating and managing promotional content, and monitoring the company's SEO automation engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's social media advertising automation campaigns, creating and managing promotional content, and monitoring the company's social media advertising automation engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's email marketing automation automation campaigns, creating and managing promotional content, and monitoring the company's email marketing automation automation engagement to ensure that the company's promotional activities are effective and engaging. The Promotions department is also responsible for managing the company's content marketing automation automation campaigns, creating and managing promotional\n"
     ]
    }
   ],
   "source": [
    "text_user_en = \"Who administrates the Promotions?\"\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "model_id = 'Llama-3.2-1B'\n",
    "model_name        = os.environ['MODEL_NAME']\n",
    "model_precision   = \"INT4\"\n",
    "inference_device  = os.environ['INFERENCE_DEVICE']\n",
    "ov_config         = {\"PERFORMANCE_HINT\":\"LATENCY\", \"NUM_STREAMS\":\"1\", \"CACHE_DIR\":cache_dir}\n",
    "num_max_tokens    = 200\n",
    "rag_chain_type    = os.environ['RAG_CHAIN_TYPE']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "ov_model_path = f'./{model_name}/{model_precision}'\n",
    "model = OVModelForCausalLM.from_pretrained(model_id=ov_model_path, device=inference_device, ov_config=ov_config, cache_dir=cache_dir)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=num_max_tokens)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=rag_chain_type, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "\n",
      "\n",
      "Question: What is the promotion policy for non teaching staff?\n",
      "Helpful Answer: The promotion policy for non teaching staff is that they are promoted based on merit. This means that they are promoted based on their performance and qualifications. The promotion policy for teaching staff is that they are promoted based on their performance and qualifications, and also based on seniority. This means that they are promoted based on their length of service and their ability to perform their job well. The promotion policy for non teaching staff is that they are promoted based on merit. This means that they are promoted based on their performance and qualifications. The promotion policy for teaching staff is that they are promoted based on their performance and qualifications, and also based on seniority. This means that they are promoted based on their length of service and their ability to perform their job well. The promotion policy for non teaching staff is that they are promoted based on merit. This means that they are promoted based on their performance and qualifications. The promotion policy for teaching staff is that they are promoted based on their performance and qualifications, and also based\n"
     ]
    }
   ],
   "source": [
    "text_user_en = \"What is the promotion policy for non teaching staff?\"\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Int 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "model_id = 'Llama-3.2-1B'\n",
    "model_name        = os.environ['MODEL_NAME']\n",
    "model_precision   = \"INT8\"\n",
    "inference_device  = os.environ['INFERENCE_DEVICE']\n",
    "ov_config         = {\"PERFORMANCE_HINT\":\"LATENCY\", \"NUM_STREAMS\":\"1\", \"CACHE_DIR\":cache_dir}\n",
    "num_max_tokens    = 100\n",
    "rag_chain_type    = os.environ['RAG_CHAIN_TYPE']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "ov_model_path = f'./{model_name}/{model_precision}'\n",
    "model = OVModelForCausalLM.from_pretrained(model_id=ov_model_path, device=inference_device, ov_config=ov_config, cache_dir=cache_dir)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=num_max_tokens)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=rag_chain_type, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "\n",
      "\n",
      "Question: Can you tell me about the staff referal incentive policy\n",
      "Helpful Answer: It is a policy in which a company gives employees a bonus for referring customers to the company. The bonus is usually a percentage of the customer's purchase amount, and the employee may also receive a small cash bonus. The policy is designed to encourage employees to refer customers to the company, as it can lead to increased sales and profits for the company. The policy is usually implemented by companies that have a high turnover rate, as it can help to retain employees who are not satisfied with their current job.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_user_en = \"Can you tell me about the staff referal incentive policy\"\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find matching results.\n"
     ]
    }
   ],
   "source": [
    "query_text = \"What is the promotion policy for non teaching staff?\"\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "\n",
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "model_id = 'Llama-3.2-1B'\n",
    "model_name        = os.environ['MODEL_NAME']\n",
    "model_precision   = \"INT8\"\n",
    "inference_device  = os.environ['INFERENCE_DEVICE']\n",
    "ov_config         = {\"PERFORMANCE_HINT\":\"LATENCY\", \"NUM_STREAMS\":\"1\", \"CACHE_DIR\":cache_dir}\n",
    "num_max_tokens    = 200\n",
    "rag_chain_type    = os.environ['RAG_CHAIN_TYPE']\n",
    "PROMPT_TEMPLATE   = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    " - -\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "# prompt = \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "ov_model_path = f'./{model_name}/{model_precision}'\n",
    "model = OVModelForCausalLM.from_pretrained(model_id=ov_model_path, device=inference_device, ov_config=ov_config, cache_dir=cache_dir)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=num_max_tokens)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "chain = prompt_template | llm\n",
    "# print(chain.invoke({\"context\": context_text,\"question\": query_text}))\n",
    "response_text = llm.predict(prompt)\n",
    "# qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=rag_chain_type, retriever=retriever, chain_type_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: What is the promotion policy for non teaching staff? \n",
      "Context:  \n",
      "Answer: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      "\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " - -\n",
      "Answer the question based on the above context: What is the promotion policy for non teaching staff?\n",
      " -\n"
     ]
    }
   ],
   "source": [
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "model_id = 'Llama-3.2-1B'\n",
    "model_name        = os.environ['MODEL_NAME']\n",
    "model_precision   = \"FP16\"\n",
    "inference_device  = os.environ['INFERENCE_DEVICE']\n",
    "ov_config         = {\"PERFORMANCE_HINT\":\"LATENCY\", \"NUM_STREAMS\":\"1\", \"CACHE_DIR\":cache_dir}\n",
    "num_max_tokens    = 200\n",
    "rag_chain_type    = os.environ['RAG_CHAIN_TYPE']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "ov_model_path = f'./{model_name}/{model_precision}'\n",
    "model = OVModelForCausalLM.from_pretrained(model_id=ov_model_path, device=inference_device, ov_config=ov_config, cache_dir=cache_dir)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=num_max_tokens)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=rag_chain_type, retriever=retriever, chain_type_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "\n",
      "\n",
      "Question: Can you tell me about the Extension of Probation Period\n",
      "Helpful Answer: The extension of the probation period is a legal process that allows a person to extend the probationary period of their sentence beyond the original length of their sentence. This can be done for a variety of reasons, such as if the person is not able to comply with the terms of their probation or if they have a change of circumstances that make it necessary to extend their probation. The length of the extension can vary depending on the circumstances, but it is typically limited to a maximum of two years.\n"
     ]
    }
   ],
   "source": [
    "text_user_en = \"Can you tell me about the Extension of Probation Period\"\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: Can you tell me about the staff referal incentive policy \n",
      "Context:  \n",
      "Answer:  \n",
      "Staff referal incentive policy is one of the main policies implemented by the company to encourage staff to refer their friends to the company. This policy allows employees to refer their friends to the company and get a percentage of the referral fee as a reward. The referral fee is calculated based on the number of referrals made by the employee. The company has a dedicated team of recruiters who are responsible for reviewing and approving the referrals made by the employees. The company also provides a support system to the employees who refer their friends to the company, such as training and resources to help them succeed. The company also has a transparent policy that allows employees to see the referrals made by their colleagues and the amount of referral fee earned by them. This policy helps in building trust and loyalty among employees and encourages them to refer their friends to the company.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_user_en = \"Can you tell me about the staff referal incentive policy\"\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: Can you tell me about the leave policy? \n",
      "Context:  \n",
      "Answer: The leave policy is designed to provide flexibility for employees to take time off when they need it. The policy allows employees to take up to 12 weeks of unpaid leave each year, with a maximum of 52 weeks in any 12-month period. This includes parental leave, sick leave, and compassionate leave. The policy also provides for a number of other benefits, such as bereavement leave, family leave, and parental bonding leave. Employees are expected to use their leave in a responsible manner and to return to work as soon as possible. If an employee is unable to return to work, they may be eligible for a short-term disability insurance plan.\n"
     ]
    }
   ],
   "source": [
    "text_user_en = \"Can you tell me about the leave policy?\"\n",
    "ans = qa_chain.run(text_user_en)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
